{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e52d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1+cu121\n",
      "Transformers: 4.56.2\n",
      "Accelerate: 1.10.1\n",
      "Bitsandbytes: 0.48.0\n",
      "\n",
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, accelerate\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# sanity checks\n",
    "bnb_version = getattr(bnb, \"__version__\", \"installed (no __version__ attribute)\")\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"Bitsandbytes:\", bnb_version)\n",
    "print(\"\\nCUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"Running on CPU only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200adc6d",
   "metadata": {},
   "source": [
    "### Hugging Face Token id is Hidded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d99447e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30247351a9c4dd387ccb90f62673b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe86137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a special sequence, each number is generated by the total number of letters when spelling the previous number in English. If the sequence begins with the number 4, what is the fifth number in the sequence?. Report the fifth number in the sequence.\n",
      "\n",
      "Answer:\n",
      "\n",
      "Step 1/2\n",
      "The first number in the sequence is 4. The second number in the sequence is 4 + 4 = 8. The third number in the sequence is 8 + 4 = 12. The fourth number in the sequence is 12 + 4 = 16. The fifth number in the sequence is 16 + 4 = 20.\n",
      "\n",
      "Step 2/2\n",
      "Therefore, the fifth number in the sequence is 20.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"In a special sequence, each number is generated by the total number of letters when spelling the previous number in English. If the sequence begins with the number 4, what is the fifth number in the sequence?. Report the fifth number\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens = 200, cache_implementation=\"static\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c38b0",
   "metadata": {},
   "source": [
    "### Gemma 2B works fine on local device.\n",
    "### No errors (like gpu out of memory) were encountered as of now. \n",
    "### For fast access here we are using 4bit Quantisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a27f0e",
   "metadata": {},
   "source": [
    "## Hence Gemma can possibly be a candidate for our AI Agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
